{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yndM7xe82EYP"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1ZPX89Y72EYV"
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "string.punctuation = string.punctuation + '“' + '”' +'-' + '’' + '‘' + '—'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "file = open('Downloads/data.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh1yCZxT2EYX",
    "outputId": "9e938f5b-6d21-4c37-e2ac-a6ee72d982b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNZJg7aP2EYY"
   },
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ccLe1sXP2EYZ"
   },
   "outputs": [],
   "source": [
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "    line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "    file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gmbl8mDM2EYZ",
    "outputId": "1d181b8e-20bf-4b7d-d3f7-8a61c8f39a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 996\n",
      "The number of tokens is 23931\n",
      "The average number of tokens per sentence is 24\n",
      "The number of unique tokens are 4741\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "words = nltk.word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb208F5J2EYa",
    "outputId": "0ff857e4-ded6-43ec-8fd9-bb24b74ae740"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAZRIE952EYb",
    "outputId": "32dc225d-ef42-4848-d2d4-17d492a7b5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('the', 1609), ('of', 896), ('and', 645), ('in', 584), ('num', 392)]\n",
      "Most common bigrams:  [(('in', 'num'), 59), (('the', 'num'), 39), (('of', 'art'), 37), (('num', 'and'), 25), (('num', 'num'), 23)]\n",
      "\n",
      "Most common trigrams:  [(('of', 'the', 'year'), 15), (('one', 'of', 'the'), 14), (('jagged', 'little', 'pill'), 13), (('the', 'num', 'century'), 11), (('of', 'the', 'num'), 11)]\n",
      "\n",
      "Most common fourgrams:  [(('on', 'num', 'june', 'num'), 8), (('on', 'the', 'same', 'day'), 5), (('in', 'the', 'united', 'states'), 5), (('in', 'the', 'early', 'num'), 5), (('administrators', 'are', 'expected', 'to'), 5)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    unigram.extend(list(ngrams(sequence,1)))\n",
    "    bigram.extend(list(ngrams(sequence, 2)))              #unigram, bigram, trigram, and fourgram models are created\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))\n",
    "\n",
    "def removal(x):                                    #removes ngrams containing only stopwords\n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)\n",
    "\n",
    "unigram = removal(unigram)\n",
    "bigram = removal(bigram)\n",
    "trigram = removal(trigram)             \n",
    "fourgram = removal(fourgram)\n",
    "\n",
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"Most common unigrams: \", freq_uni.most_common(5))      \n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))      #prints most common n-grams without add-1 smoothing and without stopword removal.\n",
    "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPKuw_wF2EYd",
    "outputId": "c526aab0-c82a-4748-bbf4-41b12d88d96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams with stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('num', 392), (('num',), 392), ('art', 160), (('art',), 160), ('turing', 119), (('turing',), 119), ('morissette', 85), (('morissette',), 85), ('also', 67), (('also',), 67)]\n",
      "\n",
      "Most common bigrams:  [(('num', 'num'), 44), ((('num',), ('num',)), 41), (('num', 'morissette'), 17), ((('num',), ('morissette',)), 17), (('num', 'turing'), 15), ((('num',), ('turing',)), 15), (('num', 'century'), 14), ((('num',), ('century',)), 14), (('june', 'num'), 14), ((('june',), ('num',)), 14)]\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#prints top 10 unigrams, bigrams after removing stopwords\n",
    "\n",
    "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
    "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
    "fdist = nltk.FreqDist(unigram_sw_removed)\n",
    "print(\"Most common unigrams: \", fdist.most_common(10))\n",
    "\n",
    "bigram_sw_removed = []\n",
    "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
    "fdist = nltk.FreqDist(bigram_sw_removed)\n",
    "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrNNavqA2EYe"
   },
   "source": [
    "## Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UvJjFgzr2EYf"
   },
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here:\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1] + total_voc[i+1])             #add-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1609)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_uni.most_common(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "f = open(\"Downloads/test.txt\", \"r\")\n",
    "preds=[]\n",
    "counter=0\n",
    "for i in f:\n",
    "    ngram[1]=list(ngrams(i,1))[-1]\n",
    "    preds.append(freq_uni.most_common(1))\n",
    "    #print(str(\"\".join(i))+\"\"+str(preds[counter][0][0]))\n",
    "    print(str(preds[counter][0][0]))\n",
    "    counter+=1\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp_assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
