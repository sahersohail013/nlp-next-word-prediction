{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed, read the dataset\n",
    "\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation + '“' + '”' +'-' + '’' + '‘' + '—'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "file = open('Downloads/corpus.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "\n",
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "    line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "    file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 981\n",
      "The number of tokens is 27361\n",
      "The average number of tokens per sentence is 28\n",
      "The number of unique tokens are 3039\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "words = nltk.word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('the', 1630), ('and', 844), ('she', 537), ('of', 508), (('said',), 462)]\n",
      "Most common bigrams:  [(('said', 'the'), 209), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59)]\n",
      "\n",
      "Most common trigrams:  [(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21)]\n",
      "\n",
      "Most common fourgrams:  [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'wont', 'you'), 8)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    unigram.extend(list(ngrams(sequence,1)))\n",
    "    bigram.extend(list(ngrams(sequence, 2)))              #unigram, bigram, trigram, and fourgram models are created\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))\n",
    "\n",
    "def removal(x):                                    #removes ngrams containing only stopwords\n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)\n",
    "\n",
    "unigram = removal(unigram)\n",
    "bigram = removal(bigram)\n",
    "trigram = removal(trigram)             \n",
    "fourgram = removal(fourgram)\n",
    "\n",
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"Most common unigrams: \", freq_uni.most_common(5))      \n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))      #prints most common n-grams without add-1 smoothing and without stopword removal.\n",
    "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams with stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [(('said',), 462), ('alice', 385), (('alice',), 385), ('little', 128), (('little',), 128), ('one', 101), (('one',), 101), ('like', 85), (('like',), 85), ('know', 85)]\n",
      "\n",
      "Most common bigrams:  [((('said',), ('alice',)), 120), (('mock', 'turtle'), 54), ((('mock',), ('turtle',)), 54), (('march', 'hare'), 31), ((('march',), ('hare',)), 31), ((('said',), ('king',)), 29), (('thought', 'alice'), 27), ((('thought',), ('alice',)), 26), (('white', 'rabbit'), 22), ((('white',), ('rabbit',)), 22)]\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#prints top 10 unigrams, bigrams after removing stopwords\n",
    "\n",
    "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
    "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
    "fdist = nltk.FreqDist(unigram_sw_removed)\n",
    "print(\"Most common unigrams: \", fdist.most_common(10))\n",
    "\n",
    "bigram_sw_removed = []\n",
    "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
    "fdist = nltk.FreqDist(bigram_sw_removed)\n",
    "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here:\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1] + total_voc[i+1])             #add-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:  {1: ('the',), 2: ('said', 'the'), 3: ('alice', 'said', 'the')}\n",
      "String 2:  {1: ('was',), 2: ('she', 'was'), 3: ('that', 'she', 'was')}\n",
      "String 3:  {1: ('album',), 2: ('next', 'album'), 3: ('your', 'next', 'album')}\n",
      "String 4:  {1: ('music',), 2: ('subsequent', 'music'), 3: ('a', 'subsequent', 'music')}\n",
      "String 5:  {1: ('helped',), 2: ('singles', 'helped'), 3: ('hit', 'singles', 'helped')}\n",
      "String 6:  {1: ('single',), 2: ('fourth', 'single'), 3: ('the', 'fourth', 'single')}\n",
      "String 7:  {1: ('respectively',), 2: ('singles', 'respectively'), 3: ('sixth', 'singles', 'respectively')}\n",
      "String 8:  {1: ('commercially',), 2: ('never', 'commercially'), 3: ('was', 'never', 'commercially')}\n",
      "String 9:  {1: ('album',), 2: ('the', 'album'), 3: ('where', 'the', 'album')}\n",
      "String 10:  {1: ('bestseller',), 2: ('a', 'bestseller'), 3: ('also', 'a', 'bestseller')}\n"
     ]
    }
   ],
   "source": [
    "#smoothed models without stopwords removed are used\n",
    "str1 = 'it was a commercial '\n",
    "str2 = 'she said of the '\n",
    "str3 = 'but the way i look at it people will like your next album '\n",
    "str4 = 'the song instantly garnered attention for its scathing explicit lyrics and a subsequent music '\n",
    "str5 = 'after the success of you oughta know the album other hit singles helped'\n",
    "str6 = 'all i really want and hand in my pocket followed but the fourth single'\n",
    "str7 = 'you learn and head over feet the fifth and sixth singles respectively'\n",
    "str8 = 'although the track was never commercially '\n",
    "str9 = 'morissette popularity grew significantly in canada where the album'\n",
    "str10= 'the album was also a bestseller'\n",
    "\n",
    "token_1 = word_tokenize(str1)\n",
    "token_2 = word_tokenize(str2)\n",
    "token_3 = word_tokenize(str3)\n",
    "token_4 = word_tokenize(str4)\n",
    "token_5 = word_tokenize(str5)\n",
    "token_6 = word_tokenize(str6)\n",
    "token_7 = word_tokenize(str7)\n",
    "token_8 = word_tokenize(str8)\n",
    "token_9 = word_tokenize(str9)\n",
    "token_10 = word_tokenize(str10)\n",
    "\n",
    "ngram_1 = {1:[], 2:[], 3:[]}                  \n",
    "ngram_2 = {1:[], 2:[], 3:[]}\n",
    "ngram_3 = {1:[], 2:[], 3:[]}                  \n",
    "ngram_4 = {1:[], 2:[], 3:[]}\n",
    "ngram_5 = {1:[], 2:[], 3:[]}                 \n",
    "ngram_6 = {1:[], 2:[], 3:[]}\n",
    "ngram_7 = {1:[], 2:[], 3:[]}                  \n",
    "ngram_8 = {1:[], 2:[], 3:[]}\n",
    "ngram_9 = {1:[], 2:[], 3:[]}                  \n",
    "ngram_10 = {1:[], 2:[], 3:[]}\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
    "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
    "    ngram_3[i+1] = list(ngrams(token_3, i+1))[-1]\n",
    "    ngram_4[i+1] = list(ngrams(token_4, i+1))[-1]\n",
    "    ngram_5[i+1] = list(ngrams(token_5, i+1))[-1]\n",
    "    ngram_6[i+1] = list(ngrams(token_6, i+1))[-1]\n",
    "    ngram_7[i+1] = list(ngrams(token_7, i+1))[-1]\n",
    "    ngram_8[i+1] = list(ngrams(token_8, i+1))[-1]\n",
    "    ngram_9[i+1] = list(ngrams(token_9, i+1))[-1]\n",
    "    ngram_10[i+1] = list(ngrams(token_10, i+1))[-1]\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"String 1: \", ngram_1)\n",
    "print(\"String 2: \",ngram_2)\n",
    "print(\"String 3: \", ngram_3)\n",
    "print(\"String 4: \", ngram_4)\n",
    "print(\"String 5: \", ngram_5)\n",
    "print(\"String 6: \", ngram_6)\n",
    "print(\"String 7: \", ngram_7)\n",
    "print(\"String 8: \", ngram_8)\n",
    "print(\"String 9: \", ngram_9)\n",
    "print(\"String 10: \", ngram_10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram and trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_1 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_1[i+1]:       #to find predictions based on highest probability of n-grams                   \n",
    "            count +=1\n",
    "            pred_1[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_1[i+1].append(\"NOT FOUND\")           #if no word prediction is found, replace with NOT FOUND\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_2[i+1]:\n",
    "            count +=1\n",
    "            pred_2[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_2[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_3 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_3[i+1]:\n",
    "            count +=1\n",
    "            pred_3[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_3[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_4 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_4[i+1]:\n",
    "            count +=1\n",
    "            pred_4[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_4[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_5 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_5[i+1]:\n",
    "            count +=1\n",
    "            pred_5[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_5[i+1].append(\"\\0\")\n",
    "            count +=1\n",
    "#6th sentence            \n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_6 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_6[i+1]:\n",
    "            count +=1\n",
    "            pred_6[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_6[i+1].append(\"\\0\")\n",
    "            count +=1\n",
    "\n",
    "#7th sentence\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "\n",
    "pred_7 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_7[i+1]:\n",
    "            count +=1\n",
    "            pred_7[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_7[i+1].append(\"\\0\")\n",
    "            count +=1\n",
    "            \n",
    "#8th sentence\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_8 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_8[i+1]:\n",
    "            count +=1\n",
    "            pred_8[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_8[i+1].append(\"\\0\")\n",
    "            count +=1\n",
    "            \n",
    "#9th\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_9 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_9[i+1]:\n",
    "            count +=1\n",
    "            pred_9[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_9[i+1].append(\"\\0\")\n",
    "            count +=1\n",
    "            \n",
    "#10th \n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_10 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_10[i+1]:\n",
    "            count +=1\n",
    "            pred_10[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_10[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
      "\n",
      "String 1 - it was a commercial-\n",
      "\n",
      "Bigram model predictions: ['arts', 'art', 'or', 'failure', 'NOT FOUND']\n",
      "Trigram model predictions: ['or', 'failure', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
      "Fourgram model predictions: ['failure', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
      "\n",
      "String 2 - she said of the-\n",
      "\n",
      "Bigram model predictions: ['num', 'same', 'first', 'album', 'year']\n",
      "Trigram model predictions: ['year', 'num', 'human', 'community', 'word']\n",
      "Fourgram model predictions: ['album', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
    "print(\"String 1 - it was a commercial-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
    "print(\"String 2 - she said of the-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_2[1], pred_2[2], pred_2[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 3 - but the way i look at it people will like your next album '-\n",
      "\n",
      "Bigram model predictions: ['was', 'by', 'alanis', 'of', 'with']\n",
      "Trigram model predictions: ['if', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 4 - the song instantly garnered attention for its scathing explicit lyrics and a subsequent music '-\n",
      "\n",
      "Bigram model predictions: ['video', 'dvd', 'and', 'for', 'producer']\n",
      "Trigram model predictions: ['video', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 5'-\n",
      "\n",
      "Bigram model predictions: ['send', 'an', '\\x00', '\\x00', '\\x00']\n",
      "Trigram model predictions: ['send', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 6 --\n",
      "\n",
      "Bigram model predictions: ['of', 'digit', 'was', 'digits', 'number']\n",
      "Trigram model predictions: ['ironic', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 7 - \n",
      "\n",
      "Bigram model predictions: ['the', 'kept', '\\x00', '\\x00', '\\x00']\n",
      "Trigram model predictions: ['kept', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 8 -\n",
      "\n",
      "Bigram model predictions: ['than', 'released', '\\x00', '\\x00', '\\x00']\n",
      "Trigram model predictions: ['released', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n",
      "String 9 - \n",
      "\n",
      "Bigram model predictions: ['was', 'by', 'alanis', 'of', 'with']\n",
      "Trigram model predictions: ['was', 'with', 'first', 'on', 'morissette']\n",
      "\n",
      "String 10 - \n",
      "\n",
      "Bigram model predictions: ['in', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "Trigram model predictions: ['in', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"String 3 - but the way i look at it people will like your next album '-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_3[1], pred_3[2]))\n",
    "\n",
    "print(\"String 4 - the song instantly garnered attention for its scathing explicit lyrics and a subsequent music '-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_4[1], pred_4[2]))\n",
    "\n",
    "print(\"String 5'-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_5[1], pred_5[2]))\n",
    "\n",
    "print(\"String 6 --\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_6[1], pred_6[2]))\n",
    "\n",
    "print(\"String 7 - \\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_7[1], pred_7[2]))\n",
    "\n",
    "print(\"String 8 -\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_8[1], pred_8[2]))\n",
    "\n",
    "print(\"String 9 - \\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_9[1], pred_9[2]))\n",
    "\n",
    "print(\"String 10 - \\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\n\" .format(pred_10[1], pred_10[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
